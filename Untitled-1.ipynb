{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- IMPORTS START --\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from scipy.signal import butter, filtfilt, find_peaks\n",
    "from sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "# -- IMPORTS END --\n",
    "\n",
    "# enable zooming into graphs\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [9, 6] # width, height in inches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do not modify\n",
    "def calc_magnitude(data):\n",
    "\n",
    "    # Calculate magnitude\n",
    "    data['accel_mag'] = np.sqrt(data['x']**2 + data['y']**2 + data['z']**2) # absolute accel magnitude\n",
    "    data['accel_mag'] = data['accel_mag'] - data['accel_mag'].mean() # detrend: \"remove gravity\"\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do not modify\n",
    "def remove_noise(data,sampling_rate):\n",
    "    from scipy.signal import butter, filtfilt, find_peaks\n",
    "\n",
    "    # Low pass filter\n",
    "    cutoff = 5 # Hz\n",
    "    order = 2\n",
    "    b, a = butter(order, cutoff/(sampling_rate/2), btype='lowpass')\n",
    "    data['filtered_accel_mag'] = filtfilt(b, a, data['accel_mag'])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_peaks(data,sampling_rate,ht,pm,dist):\n",
    "    # detect peaks\n",
    "    peak_indices = find_peaks(data['accel_mag'], height=ht, prominence=pm, distance=dist * sampling_rate)[0]\n",
    "    peaks = data['accel_mag'][peak_indices]\n",
    "\n",
    "    # add new column\n",
    "    data['peaks'] = peaks\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(window):\n",
    "    features = {}\n",
    "    features['avg'] = window['filtered_accel_mag'].mean()\n",
    "    features['max'] = window['filtered_accel_mag'].max()  # Using max() instead of quantile\n",
    "    features['med'] = window['filtered_accel_mag'].median()  # Using median() for the median value\n",
    "    features['min'] = window['filtered_accel_mag'].min()  # Using min() for the minimum value\n",
    "    features['q25'] = window['filtered_accel_mag'].quantile(0.25)\n",
    "    features['q75'] = window['filtered_accel_mag'].quantile(0.75)\n",
    "    features['std'] = window['filtered_accel_mag'].std()\n",
    "    \n",
    "    return features\n",
    "\n",
    "def find_peaks_in_data(data, sensor_type, sampling_rate, ht=0.2, pm=0.1, dist=0.1):\n",
    "    # Select the appropriate signal (accelerometer or gyroscope)\n",
    "    if sensor_type == 'accelerometer':\n",
    "        signal = data['filtered_accel_mag']  # For accelerometer, use accel_mag\n",
    "    elif sensor_type == 'gyroscope':\n",
    "        signal = data['filtered_gyro_mag']  # For gyroscope, use gyro_mag\n",
    "\n",
    "    # Find peaks\n",
    "    peak_indices, _ = find_peaks(signal, height=ht, prominence=pm, distance=dist * sampling_rate)\n",
    "    peaks = signal.iloc[peak_indices]\n",
    "    \n",
    "    # Return peak indices and peak values\n",
    "    return peak_indices, peaks\n",
    "\n",
    "def add_peak_features(data, sensor_type, sampling_rate):\n",
    "    \"\"\"Detect peaks and extract peak-related features\"\"\"\n",
    "    # Detect peaks in the specified sensor data\n",
    "    peak_indices, peaks = find_peaks_in_data(data, sensor_type, sampling_rate)\n",
    "\n",
    "    # Add peak features to the feature dictionary\n",
    "    peak_features = {}\n",
    "    if len(peaks) > 0:\n",
    "        peak_features['peak_count'] = len(peaks)  # Number of detected peaks\n",
    "        peak_features['avg_peak_height'] = peaks.mean()  # Average height of the peaks\n",
    "        peak_features['max_peak_height'] = peaks.max()  # Maximum peak height\n",
    "\n",
    "        # Calculate time between consecutive peaks\n",
    "        peak_times = data.iloc[peak_indices].index\n",
    "        peak_intervals = peak_times.diff().dropna().total_seconds()  # Time difference between consecutive peaks\n",
    "        if len(peak_intervals) > 0:\n",
    "            peak_features['avg_time_between_peaks'] = peak_intervals.mean()  # Average time between peaks\n",
    "            peak_features['std_time_between_peaks'] = peak_intervals.std()  # Std dev of time between peaks\n",
    "        else:\n",
    "            peak_features['avg_time_between_peaks'] = np.nan\n",
    "            peak_features['std_time_between_peaks'] = np.nan\n",
    "    else:\n",
    "        # If no peaks are detected, set the features to NaN or 0\n",
    "        peak_features['peak_count'] = 0\n",
    "        peak_features['avg_peak_height'] = np.nan\n",
    "        peak_features['max_peak_height'] = np.nan\n",
    "        peak_features['avg_time_between_peaks'] = np.nan\n",
    "        peak_features['std_time_between_peaks'] = np.nan\n",
    "\n",
    "    return peak_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data, window_sec, sample_rate):\n",
    "    if 'time' in data.columns:\n",
    "        data['time'] = pd.to_datetime(data['time'])\n",
    "        data = data.set_index('time')\n",
    "\n",
    "    frame_list = []\n",
    "    sample_count = int(window_sec * sample_rate)\n",
    "\n",
    "    for t, window in data.resample(f\"{window_sec}S\"):\n",
    "        if len(window) < sample_count:\n",
    "            continue\n",
    "\n",
    "        # Extract basic features (avg, max, min, etc.)\n",
    "        basic_features = add_features(window)\n",
    "\n",
    "        # Extract peak-related features for accelerometer\n",
    "        accel_peak_features = add_peak_features(window, 'accelerometer', sample_rate)\n",
    "\n",
    "        # Extract peak-related features for gyroscope\n",
    "        gyro_peak_features = add_peak_features(window, 'gyroscope', sample_rate)\n",
    "\n",
    "        # Combine the features\n",
    "        frame = {**basic_features, **accel_peak_features, **gyro_peak_features}\n",
    "\n",
    "        # Add time and other metadata\n",
    "        frame['time'] = t\n",
    "\n",
    "        frame_list.append(frame)\n",
    "\n",
    "    resampled_data = pd.DataFrame(frame_list)\n",
    "    return resampled_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_data_to_combined_csv(root, output_filename='pilates_data2.csv'):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    files = glob.glob(os.path.join(root, '**', '*.csv'), recursive=True)\n",
    "\n",
    "    for file in files:\n",
    "        # Determine sensor type\n",
    "        if 'accelerometer' in file.lower():\n",
    "            sensor_type = 'accelerometer'\n",
    "        elif 'gyroscope' in file.lower():\n",
    "            sensor_type = 'gyroscope'\n",
    "        else:\n",
    "            print(f\"Skipping unknown file type: {file}\")\n",
    "            continue\n",
    "\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        # Calculate magnitude and remove noise for accelerometer data\n",
    "        sampling_rate = 100  # Adjust based on your actual sampling rate\n",
    "        window_sec = 2  # Set desired time window for feature extraction\n",
    "        df = calc_magnitude(df)\n",
    "        df = remove_noise(df, sampling_rate)\n",
    "\n",
    "        # Extract metadata from file path\n",
    "        parts = os.path.normpath(file).split(os.sep)\n",
    "        exercise = parts[-3]\n",
    "        pace_raw = parts[-2]\n",
    "        pace = pace_raw.split('-')[0]\n",
    "\n",
    "        # Extract features (including peak features)\n",
    "        features = extract_features(df, window_sec=window_sec, sample_rate=sampling_rate)\n",
    "\n",
    "        # Add peak-related features (for both accelerometer and gyroscope)\n",
    "        if sensor_type == 'accelerometer':\n",
    "            accel_peak_features = add_peak_features(df, 'accelerometer', sampling_rate)\n",
    "            features = {**features, **accel_peak_features}  # Merge peak features into main features\n",
    "        elif sensor_type == 'gyroscope':\n",
    "            gyro_peak_features = add_peak_features(df, 'gyroscope', sampling_rate)\n",
    "            features = {**features, **gyro_peak_features}  # Merge peak features into main features\n",
    "\n",
    "        # Add metadata to the features DataFrame\n",
    "        features['exercise'] = exercise\n",
    "        features['pace'] = pace\n",
    "        features['sensor_type'] = sensor_type\n",
    "\n",
    "        # Append features to all_data\n",
    "        all_data = pd.concat([all_data, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "    # Save the combined data to a CSV\n",
    "    output_path = os.path.join(root, output_filename)\n",
    "    all_data.to_csv(output_path, index=False)\n",
    "    print(f\"Combined data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_tree(frames):\n",
    "    # Include both basic and peak-related features\n",
    "    X = frames[['avg', 'max', 'med', 'min', 'q25', 'q75', 'std', \n",
    "                'peak_count', 'avg_peak_height', 'max_peak_height', \n",
    "                'avg_time_between_peaks', 'std_time_between_peaks']]\n",
    "\n",
    "    # Extract target column (pace)\n",
    "    y = frames['pace']\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Create model\n",
    "    dt_model = DecisionTreeClassifier(criterion='entropy', max_depth=5)\n",
    "    dt_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    dt_pred = dt_model.predict(X_test)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    acc = dt_model.score(X_test, y_test)\n",
    "\n",
    "    # Print classification report\n",
    "    print(classification_report(y_test, dt_pred))\n",
    "    print(\"Accuracy on test set:\", acc)\n",
    "\n",
    "    return dt_model, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/pilates_data2.csv')\n",
    "\n",
    "data['pace'] = data['pace'].map({'correct': 0, 'incorrect': 1})\n",
    "feature_columns = ['avg', 'max', 'med', 'min', 'q25', 'q75', 'std']\n",
    "data[feature_columns] = data[feature_columns].apply(\n",
    "    lambda x: x.str.replace(r'^\\d+\\s+', '', regex=True) \n",
    "                .str.replace(r'\\nName:.*', '', regex=True) \n",
    "                .astype(float)\n",
    ")\n",
    "\n",
    "print(data[feature_columns].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs328",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
